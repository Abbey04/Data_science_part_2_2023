{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 and 3: End-to-end Supervised Learning Project (Regression)\n",
    "\n",
    "# Week 2: Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.8 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, )\n",
    "\n",
    "# Scikit-Learn ≥1.0 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Precision options\n",
    "np.set_printoptions(precision=2)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "\"\"\"\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\"\"\"\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the Data\n",
    "\n",
    "First of all let's import the data from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.abspath(os.path.join('..', 'datasets', 'kings_county_house_data.csv'))\n",
    "print('File path: {}'.format(filepath))\n",
    "housing = pd.read_csv(filepath, dtype={'zipcode': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an overall idea of the fields available using the `DataFrame.info()` and `DataFrame.describe()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Training and Test Set\n",
    "\n",
    "Creating a test set is theoretically simple: pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside. We will use a function from `scikit-learn` which splits a dataset into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we would just be doing a simple randomized sampling. But this might not be a representative sampling of the whole dataset, if we do not preserve the proportions (or percentages) of significant input features. Let's hypothesize that we learned from expert the `sqft_living` field is an important predictor for the house price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.sqft_living.hist(bins=100, figsize=(14,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"sqft_living_cat\"] = pd.cut(housing.sqft_living, \n",
    "                                    bins=[0., 1000., 2000., 3000., 4000., np.inf],\n",
    "                                    labels=[1, 2, 3, 4, 5]\n",
    "                                   )\n",
    "housing['sqft_living_cat'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['sqft_living_cat'].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in splitter.split(housing, housing.sqft_living_cat):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.shape, strat_test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.sqft_living_cat.value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set.sqft_living_cat.value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"sqft_living_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Outlier Detection (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = strat_train_set[\n",
    "    ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', \n",
    "     'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "].plot.box(subplots=True, layout=(3, 3), figsize=(18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set[['price']].boxplot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot use the IQR method to display data and outliers(shape of the data) but in order to be get a list of identified outlier, we will need to use the mathematical formula and retrieve the outlier data.\n",
    "\n",
    "Wikipedia Definition:\n",
    "_The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, $IQR = Q_3 − Q_1$.\n",
    "\n",
    "In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.\n",
    "\n",
    "It is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers._\n",
    "\n",
    "If a data point is below $Q_1 - 1.5\\times IQR$ or above $Q_3 + 1.5\\times IQR$ then it's an outlier.\n",
    "\n",
    "<b>Exercise 1:</b> Compute for me the count of outliers in our training set with respect to the `price` feature. (Hint: check the `DataFrame.quantile()` method and find a way to count the occurrences of values in a column of a DataFrame.) Additionally, write the code to remove those outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write you solution here. Add as many cells as you see fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the outliers legitimate or should we remove them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize geographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.plot(\n",
    "    kind=\"scatter\", x=\"long\", y=\"lat\", figsize=(15,10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.plot(kind=\"scatter\", x=\"long\", y=\"lat\", alpha=0.1, figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.plot(\n",
    "    kind=\"scatter\", x=\"long\", y=\"lat\", alpha=0.1, figsize=(20,13),\n",
    "    s=strat_train_set[\"sqft_living\"]/100, label=\"sqft_living\",\n",
    "    c=\"price\", cmap=plt.get_cmap('jet'), colorbar=True\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same colormap (i.e. jet), we can try to improve the visualization above, setting an upper value that is reasonable, (i.e less or equal to QR3 + 1.5 IQR such as 1,000,000 $), and not the highest value in the range.\n",
    "\n",
    "We can create a custom discrete colorbar by using `matplotlib.colors.BoundaryNorm` as normalizer for your scatterplot. See the norm argument in `matplotlib.pyplot.scatter()`: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.jet  # define the colormap\n",
    "bounds = np.linspace(0, 1e6, 11) # define 11 evenly space\n",
    "\n",
    "# The matplotlib.colors.BoundaryNorm class is used to create a colormap based on discrete numeric intervals.\n",
    "norm = mpl.colors.BoundaryNorm(\n",
    "    bounds, # Monotonically increasing sequence of boundaries\n",
    "    cmap.N # Number of colors in the colormap to be used\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 13))\n",
    "plt.scatter(\n",
    "    x=strat_train_set[\"long\"],\n",
    "    y=strat_train_set[\"lat\"],\n",
    "    alpha=0.1,\n",
    "    s=strat_train_set[\"sqft_living\"]/100, # size of the dot\n",
    "    label=strat_train_set[\"sqft_living\"],\n",
    "    c=strat_train_set[\"price\"], # colour of the dot\n",
    "    cmap=cmap, # colour map \n",
    "    norm=norm # used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap\n",
    ")\n",
    "plt.colorbar(label=\"Price\", orientation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.jet  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# force the first color entry to be grey\n",
    "cmaplist[0] = (.5, .5, .5, 1.0)\n",
    "\n",
    "# create the new map\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N\n",
    ")\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0, 1e6, 11)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "plt.figure(figsize=(20, 13))\n",
    "plt.scatter(\n",
    "    x=strat_train_set[\"long\"], y=strat_train_set[\"lat\"],\n",
    "    alpha=0.1,\n",
    "    s=strat_train_set[\"sqft_living\"]/100, label=strat_train_set[\"sqft_living\"],\n",
    "    c=strat_train_set[\"price\"], cmap=cmap, norm=norm\n",
    ")\n",
    "plt.colorbar(label=\"Price\", orientation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2:</b> explore on your own other ways to improve the graph above. You could look for ways to overlap it on top of the county map, or you could see if you can encode information differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Looking for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not that big, and we can compute the standard correlation coefficient (Pearson’s r coefficient) between every two features using the `DataFrame.corr()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = strat_train_set.corr()\n",
    "corr_matrix[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. The correlation coefficient only measures linear correlations, and it may completely miss nonlinear correlation factors. \n",
    "\n",
    "Another way to check for correlation visually is to use the `scatter_matrix()` utility function offered by Pandas, which leverages `matplotlib`, or seaborn's `pairplot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"price\", \"sqft_living\", \"grade\",\n",
    "              \"sqft_above\", \"sqft_living15\", \"bathrooms\"]\n",
    "pd.plotting.scatter_matrix(\n",
    "    strat_train_set[attributes], figsize=(15, 10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.pairplot(strat_train_set[attributes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then start using boxplots or violinplots to further investigate targeted correlations, such as 'grade' vs 'price' or 'floors' vs 'price'.\n",
    "\n",
    "<b>Exercise 3:</b> write a function that takes a categorical or ordinal feature as a first argument, the size of a figure as a second argument and plots, using seaborn, a set of boxplots of the price distribution for each category in the input categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write you solution here.\n",
    "# Populate the function body. I've written the function interface (i.e. its API) for you\n",
    "def print_boxplot(feature, figsize=(14, 6)):\n",
    "    \"\"\"\n",
    "    write your function here, removing the \"pass\" statement\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your function to plot 'grade' vs 'price'.\n",
    "## If you have implemented it correctly it will plot out the boxplots\n",
    "print_boxplot('grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your function to plot 'floors' vs 'price'.\n",
    "## If you have implemented it correctly it will plot out the boxplots\n",
    "print_boxplot('floors', figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your function to plot 'bathrooms' vs 'price'.\n",
    "## If you have implemented it correctly it will plot out the boxplots\n",
    "print_boxplot('bathrooms', figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Data Pre-processing and Regression Algorithms\n",
    "\n",
    "## 4. Preparing the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"price\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"price\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Missing values and imputation\n",
    "\n",
    "We can use `pd.DataFrame.isna()` or `pd.DataFrame.isnull()` to look for null or missing values in any of our variables/features.\n",
    "\n",
    "NOTE: axis=1 performs the operation along the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look for rows with incomplete values\n",
    "incomplete_rows = housing[housing.isna().any(axis=1)]\n",
    "incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, imputation is the process of replacing missing data with substituted values.\n",
    "In scikit-learn we can use the `SimpleImputer` calss to perform imputation on missing  values. Generally we wil want to replace missing numeric (quantitative) values with the median value of that feature. For categorical features we may want to either use a \"missing\"/\"unknown\" category, use the mode, or drop the samples with missing values.\n",
    "\n",
    "As there is no missing values in our dataset we would not really need to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_num)\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing.index)\n",
    "housing_tr.sort_values(by='bathrooms', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[[14423, 1149, 6994]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to look for weird values as well...as it can be seen from the boxplotof bathrooms vs price some houses seem to have no bathroom. This is most likely an error and we may wont to replace those values with the median values for for bathrooms as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[housing['bathrooms'] == 0, 'bathrooms'] = np.nan\n",
    "housing_tr.bathrooms.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.bathrooms.fillna(housing_tr.bathrooms.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[[14423, 1149, 6994]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transform Rows\n",
    "\n",
    "It is always a good practice to write functions (or classes) to perform data transformations, so that these operations can later be performed in a reliable and reproducible way.\n",
    "\n",
    "Below, we have a method that replaces zeroes with the median values for bathrooms, and splits the date the house was sold into `month_bought` and `yr_bought`, then computes the `age` (in years) of teh house when it was sold and introduces a boolean flag (`renovated_flag`) that states whether the house was renovated or not before being sold. It then removes features deemed not relevant (such as `id`, `date`, `yr_built`, `yr_bought`, `month_bought`, `yr_renovated`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_housing_rows(housing_df):\n",
    "    \"\"\"\n",
    "    cleans up the data \n",
    "    \"\"\"\n",
    "    res = housing_df.copy()\n",
    "    # houses with 0 bathrooms are a mistake, set them as null values\n",
    "    res.loc[housing_df['bathrooms'] == 0, 'bathrooms'] = np.nan\n",
    "    # use median imputing for missing values in bathrooms\n",
    "    res.bathrooms.fillna(res.bathrooms.median(), inplace=True)\n",
    "    # split the acquisition date in year and month and compute the age of the house when bought\n",
    "    if 'date' in res.columns.values:\n",
    "        res['yr_bought'] = res['date'].apply(lambda d: int(d[:4]))\n",
    "        res['month_bought'] = res['date'].apply(lambda d: int(d[4:6]))\n",
    "        res['age'] = res['yr_bought'] - housing_df['yr_built']\n",
    "    # create a boolean flag to see whether the house was renovated or not\n",
    "    res['renovated_flag'] = res['yr_renovated'].apply(lambda el: 0 if el == 0.0  else 1)\n",
    "    # drop not relevant columns\n",
    "    return housing_df.drop(\n",
    "        columns=['id', 'date', 'yr_built', 'yr_bought', 'month_bought', 'yr_renovated'],\n",
    "        errors='ignore'\n",
    "    )\n",
    "    \n",
    "housing_proc = transform_housing_rows(housing)\n",
    "housing_proc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handling Text and Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we can consider three types of text features: categorical, ordinal, and unstructured.\n",
    "Unstructured text is more the subject of Natural Language Processing, hence we will not consider its processing/encoding at this stage (and we have no unstructured data). Ordinal data are text categories that imply and intrinsic order such as the set (\"BAD\", \"AVERAGE\", \"GOOD\", \"VERY GOOD\", \"EXCELLENT\"). These are generally encodes as integers (\"BAD\" => 0, \"AVERAGE\" => 1, \"GOOD\" => 2, \"VERY GOOD\" => 3, \"EXCELLENT\" => 4). These transformations can be handed with custom functions as above.\n",
    "\n",
    "To handle Categorical Attributes that are not ordinal, a common solution is to create one binary attribute per category. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called *dummy attributes*. Scikit-Learn provides a `sklearn.preprocessing.OneHotEncoder` class to convert categorical values into one-hot vectors.\n",
    "\n",
    "In our case we have the \"zipcode\" attribute that can be considered as categorical. Each \"zipcode\" category should become a mutually exclusive dummy attribute\n",
    "\n",
    "<b>Exercise 4:</b> Use the `OneHotEncoder` class to encode each ZIP code as a separate category. Check the documentation for appropriate use of the `OneHotEncoder` transformer. What kind of output do you get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution here\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Custom Transformers (see if we should do this one)\n",
    "\n",
    "You can define you own transformers creating a class that inherits both from `BaseEstimator` (so that it inherits the `.fit()` method) and the mixin class `TransformerMixin` (so that it acquires the `.tranform()` method)\n",
    "\n",
    "Here below you can see a transformer that set two new features: `renovated_flag` and `age`.\n",
    "\n",
    "Transformers can take DataFrames as inputs but always output NumPy arrays, so for some transformations where you find more convenient to work on DataFrame you might prefer just to write you own functions that return a modified DataFrame as we did with `transform_housing_rows()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "yr_built_ix = housing.columns.values.tolist().index('yr_built') \n",
    "yr_renovated_ix = housing.columns.values.tolist().index('yr_renovated')\n",
    "print(yr_built_ix, yr_renovated_ix)\n",
    "\n",
    "class ManipulatedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    CURRENT_YEAR = 2017\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        age = self.CURRENT_YEAR - X[:, yr_built_ix]\n",
    "        renovated_flag = np.apply_along_axis(lambda r: 0 if r[yr_renovated_ix] == 0 else 1, 1, X)\n",
    "        return np.concatenate((X, age[:, np.newaxis], renovated_flag[:, np.newaxis]), axis=1)\n",
    "\n",
    "attr_adder = ManipulatedAttributesAdder()\n",
    "housing_extra_attribs = pd.DataFrame(\n",
    "    attr_adder.transform(housing_num.values),\n",
    "    columns=[*housing_num.columns.values.tolist(), 'age', 'is_renovated'],\n",
    "    index=housing_num.index\n",
    ")\n",
    "housing_extra_attribs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to perform this transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations you need to apply to your data is feature scaling. In the great majority of case, Machine Learning algorithms will not perform well when the input numerical attributes have very different scales.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale:\n",
    "* min-max scaling:  rescaling the range of features to scale the range in [0, 1] or [−1, 1] (using scikit-learn `MinMaxScaler`)\n",
    "* standardization: scales the data to have zero mean and variance = 1 (using scikit-learn `StandardScaler`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Transformation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are goin to use a `StandardScaler` directly after our `ManipulatedAttributesAdder` class using a Tranformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "housing_num_tr = transform_housing_rows(housing_num)\n",
    "housing_num_prepared = num_pipeline.fit_transform(housing_num_tr)\n",
    "housing_num_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared.min(axis=1), housing_num_prepared.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have handled the categorical columns and the numerical columns separately. It would be more convenient if we had just one transformer capable to handle all columns, applying the appropriate transformations to each column. Solution: we can use scikit-learn `ColumnTransformer`!\n",
    "\n",
    "<b>Exercise 5:</b> Use the `sklearn.compose.ColumnTransformer` class to create a pipeline that processes separately the numeric/quantitative attributes (using the `num_pipeline` that we have defined above) and the Categorical Atrribute `zipcode` using a one-hot-encoding without scaling. The binary attributes should remain unmodified\n",
    "\n",
    "Hint: check the `ColumnTransformer` API: https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution here:\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "housing_tr = transform_housing_rows(housing)\n",
    "\n",
    "\n",
    "\n",
    "cat_attribs = [\"zipcode\"]\n",
    "binary_attributes = [\"renovated_flag\", \"waterfront\", \"view\"] \n",
    "num_attribs = [\n",
    "    el for el in list(\n",
    "        housing_tr.select_dtypes(include=[np.number])\n",
    "    ) if el not in binary_attributes\n",
    "]\n",
    "print(num_attribs)\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(categories='auto'), cat_attribs),\n",
    "        # (\"bin\", None, binary_attributes)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OneHotEncoder` returns a sparse matrix, while `num_pipeline` returns a dense matrix. `ColumnTransformer` estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the density is lower than a given threshold. In this example, it returns a sparse matrix. \n",
    "\n",
    "And now we are done! We have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(housing_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all the pre-processed data (if needed, skipped in the class demo)\n",
    "\n",
    "We will use `scipy.sparse.save_npz()` to save the sparse matrix, `np.save()` to save the NumPy arrays as `.npy` binary files, and `pickle` from the Python standard library to save our `full_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "X_filepath = os.path.abspath(os.path.join('..', 'dumps', 'kc_house_data_prepared.npz'))\n",
    "y_filepath = os.path.abspath(os.path.join('..', 'dumps', 'kc_housed_data_prediction.npy'))\n",
    "train_set_filepath = os.path.abspath(os.path.join('..', 'dumps', 'kc_housed_data_train_set.csv'))\n",
    "test_set_filepath = os.path.abspath(os.path.join('..', 'dumps', 'kc_housed_data_test_set.csv'))\n",
    "pipeline_filepath = os.path.abspath(os.path.join('..', 'dumps', 'full_preprocessing_pipeline'))\n",
    "\n",
    "# create the \"dumps\" directory if it does not exist\n",
    "os.makedirs(os.path.abspath(os.path.join('..', 'dumps')), exist_ok=True)\n",
    "\n",
    "with open(pipeline_filepath, 'wb') as file_handle:\n",
    "    pickle.dump(full_pipeline, file_handle)\n",
    "    \n",
    "print(type(strat_train_set), type(strat_test_set))\n",
    "\n",
    "save_npz(X_filepath, housing_prepared)\n",
    "np.save(y_filepath, housing_labels)\n",
    "strat_train_set.to_csv(train_set_filepath)\n",
    "strat_test_set.to_csv(test_set_filepath)\n",
    "# np.save(train_set_filepath, strat_train_set)\n",
    "# np.save(test_set_filepath, strat_test_set)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "some_data = strat_train_set.iloc[:10]\n",
    "some_labels = strat_test_set.iloc[:10]\n",
    "preprocessed_some_data = full_pipeline.transform(some_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select and train a model - Regression\n",
    "\n",
    "\n",
    "We will start by looking at the Linear Regression model, the simplest Regression model there is. There are two different ways to train it:\n",
    "\n",
    "* Using a direct “closed-form” equation that directly computes the model parameters that best fit the model to the training set (i.e., the model parameters that minimize the cost function over the training set).\n",
    "\n",
    "* Using an iterative optimization approach called Gradient Descent (GD) that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of Gradient Descent: Batch GD, Mini-batch GD, and Stochastic GD. This will be used again later on, when we will be seeing Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we will only use the numerical fields and discard the categorical field \"zipcode\"\n",
    "housing_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_housing_rows cleans-up and feature-engineers our dataset\n",
    "housing_num_tr = transform_housing_rows(housing_num)\n",
    "\n",
    "# num_pipeline does (1) imputing: replaces missing values with the median value for the feature\n",
    "# and (2) standardizes features by removing the mean and scaling to unit variance.\n",
    "housing_prepared = num_pipeline.fit_transform(housing_num_tr)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Linear Regression - Closed form solution: Normal Equation \n",
    "\n",
    "We will use the `LinearRegression` class from `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try it out on a few instances from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing_num.iloc[:10]\n",
    "some_labels = housing_labels.iloc[:10]\n",
    "preprocessed_some_data = num_pipeline.transform(\n",
    "    transform_housing_rows(some_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions:\", lin_reg.predict(preprocessed_some_data))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It mostly works, although the predictions are not exactly accurate. Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s `mean_squared_error()` function and computing its square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Linear Regression -  Gradient Descent\n",
    "\n",
    "Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The concept behind Gradient Descent is to modify parameters iteratively to minimize a cost function.\n",
    "\n",
    "The MSE cost function for a Linear Regression model is a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Batch Gradient Descent\n",
    "\n",
    "No example on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Stochastic Gradient Descent\n",
    "\n",
    "To perform Linear Regression using Stochastic GD with Scikit-Learn, you can use the `SGDRegressor` class. It defaults to optimizing the squared error cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(\n",
    "    loss=\"squared_error\", # default cost function (MSE)\n",
    "    max_iter=1000,   # maximum number of passes over the training data (aka epochs)\n",
    "    penalty=None, # no regularisation\n",
    "    eta0=0.01,  # initial learning rate\n",
    "    tol=1e-3,   # stopping criterion tolerance. stop searching for a minimum \n",
    "                # (or maximum) once some tolerance is achieved, i.e. \n",
    "                # once you're close enough.\n",
    ")\n",
    "sgd_reg.fit(housing_prepared, housing_labels)\n",
    "print('SGD Regressor intercept: {}'.format(sgd_reg.intercept_))\n",
    "print('SGD Regressor coefficient: {}'.format(sgd_reg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = sgd_reg.predict(housing_prepared)\n",
    "sgd_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "sgd_rmse = np.sqrt(lin_mse)\n",
    "sgd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "\n",
    "Learning curves are a way to check whether your algorithm is learning properly. They are plots of the model's performance on the train and validation set as a function of the number of training iterations (or the size of the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import seaborn as sns\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def plot_learning_curves(model, X, y, max_training_samples=None):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2\n",
    "    )\n",
    "    train_errors, val_errors = [], []\n",
    "    # for training set size from 1 to max_training_samples\n",
    "    for m in range(1, min(len(X_train), max_training_samples)):\n",
    "        # train a new model\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        # make predictions on the training and validation set\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        # compute the MSE on the training and validation set\n",
    "        train_errors.append(\n",
    "            mean_squared_error(y_train[:m], y_train_predict)\n",
    "        )\n",
    "        val_errors.append(\n",
    "            mean_squared_error(y_val, y_val_predict)\n",
    "        )\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    # plot our performance metric (i.e. the RMSE)\n",
    "    sns.lineplot(data=np.sqrt(train_errors), linewidth=2, label=\"train\", ax=ax)\n",
    "    sns.lineplot(data=np.sqrt(val_errors), linewidth=2, label=\"val\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_learning_curves(\n",
    "    sgd_reg, housing_prepared, housing_labels, max_training_samples=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation set actually have the same RMSE for $n_{samples} > 100$ but we can see that the RMSE flattens and does not keep decreasing. This means that the model is underfitting the training data.\n",
    "\n",
    "If the model were overfitting the data you would notice a gap between the training and the validation curve (i.e. the validation error would be consistently greater than the training error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 6:</b> Initialize and train a `SGDRegressor` named `sgd_reg` with initial learning rate of $0.005$, `penalty` set to `None`, training for a maximum of 500 epochs over all the dataset. Then, pass the model and the input data and the output labels to the `plot_learning_curves()` function, setting its `max_training_samples` argument to $5000$. What is it plotting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Regression"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAADZCAIAAAA4xbWGAAAgAElEQVR4Ae1dB1gUxxefq5SDo2OhIyJFwa6AHQtiLMGGBUSN2DGxJCYxlpjkbzeWKGIsWKNYiA0jdmJJbCggSKQjnaNd49r+v2HhPO6Ofo1z5rsPZmfezLz57b6dnZk37xEwDAMoIAQQAlqBAFEreoE6gRBACEAEkDyj5wAhoD0IIHnWnnuJeoIQQPKMngGEgPYggORZe+4l6glCAMkzegYQAtqDAJJn7bmXqCcIASTP6BlACGgPAkietedeop4gBJA8o2cAIaA9CCB51p57iXqCEEDyjJ4BhID2IIDkWXvuJeoJQgDJM3oGEALagwCSZ+25l6gnCAEkz+gZQAhoDwJInrXnXqKeIASQPKNnACGgPQggedaee4l6ghBA8oyeAYSA9iCA5Fl77iXqCUIAyTN6BhAC2oOAguX59u3bX3zxxbJly7KysgAApaWlX3311ddff81kMrUHM9QThICmIkBQrP3tN2/eODs75+bmrl69Ojo6eu7cud98801ZWVlUVNSuXbs0FQTEF0JASxAgK7YfHh4eAAAul2tmZgYA+PDhg4uLCwBgw4YNeEMYht25cwcAkJKS4urqqtjWUW0IAU1GgMFgTJ06VakcKlieAQCFhYXffvvtkSNHAAAikQjnXvIroKysDAAQFRW1ZMkSpfYNVY4Q0CgETpw40c7kuaSkJDQ09MCBA5aWlgAAPT09NpvNYrHMzc1xZAkEAt6l2NjY6dOnaxTciBmEgFIRwL9MldqEgsfnjRs3MhiMVatW2dnZbd++fe3atcHBwRiGrV+/XqndQJUjBBACAAAFy/P+/fslYfWpCZIpUnGhUEgikaQS0SVCACHQOgQULM8tYuL169exsbG6urqhoaFUKrVFZRExQgAhIIuAgvefZRtoJOXly5csFovNZhcUFDRChrIQAgiBZiKgTnkeNmyYsbGxpaWllZVVM9lFZAgBFSOQl5fH5XJV3Girm1OnPDs4OKxYsSIkJARNoVt9/1DBViBw8eLFkJCQXr16BQQEhISEMBgM2UqioqLevn0LADhw4EBGRoYsgWamqHP+rJmIIK60HoHJNWH58uULFy6srq7+888/X7x4ERwczOPxBg0adOXKlX79+h09etTc3HzixIkAgDt37vz2228BAQEjRozQcHCQPGv4DULsSSNQXAyWLpVObPz65EmgoyOfJD09/enTp4cOHYqJicnNzR00aNDz58+dnJx8fX29a0J8fLyent6uXbv8/f2RPMsHEaUiBFqNAJUK+vRpWWlio9PKIUOGAAAIBAJeqVipUdyGl5cXlUo1NjYWp2hsBI3PGntrEGPyETAyAt98Iz+rdanEGnHv0KHD9evX+Xz+kydPZs6cSaPRsrOzvb29JUW9dfWrslSjLy5VMoLaQgioFgFfX19zc3MXFxc3NzcAQK9evRwcHL7//vv58+ebm5vPmDEjLS3twoULgwcPxg8XTZo0SbUMtqY1BZ+XbD4LoaGhERERzadHlAiB9o6ACp55ND6394cE8Y8Q+IgAkuePWKAYQqC9I4Dkub3fQcQ/QuAjAmqVZwwDV6585AXFEAIagwCfz6+oqNAYdprLiFrluaICLFwIIiObyyyiQwgoCIGpU6eG1ITdu3dLVllUVHTixAkAQF5eXnR0tGRWQ/H8/PwzZ840lKvidLXuPxsbg4gIEBQEfH2BtbWKe46a+5QRKC0tjYqKwhEQCoW7d+/OzMwcP358Tk5OREREenr6kiVLrK2tKysrr127lpyc7OTk5OnpefLkyWnTpg0YMODKlSuxsbHOzs5LliyJjo4+ceJEWlraDz/8cPXq1Tt37vj6+o4fP14t8KpVngEA48eDcePAokXg2jW19B812u4QKOWUfnfnuxaxvXfsXh1SPYVPoVAYEhICABg+fLiNjQ2Tydy/f39xcbGzs3NKSsrGjRuTkpIePHjg4uLy66+/3rt3b+nSpaWlpZs2bQoICLh161avXr0mTJjw+++/R0dH+/r6Zmdn//DDD48fP/7777937doVFhbm4uLStWvXFjGpEGJ1yzMAYO9e0KMHOHQIfnujgBBoCgEigWiia9IUVb18AqjV5RSnkkik48eP45dVVVXR0dEzZ85cuHChra2tmAaP9O/fn0ajeXp69unTx6Am4IeuGAwGh8Pp3r17jx49cMqnT59mZ2d/9913PB6vqKjoU5VnMzNw/DiYPBkMHQpqjPtKAYouEQKSCJjommwZuUUypXVxPp8PACASiSKRaNeuXVwuNyAgIDIysri4WLJCXBtUrPWJq3m/ePHi1q1bO3bsEAgEenp6JSUlAAAPD4+ysrLNmzez2WyxNrhkVSqIa8D4DAAYPRrMmwdmzQJPngBkeEgFt/2Tb8LPzw83F+3k5PTZZ58dPHhQIBBs2rSpU6dOffv2Xbdu3ZIlS/r27Uuj0XAV7h49enTq1AkAMGrUKABAYGDg0qVLe/fubWVlZWNj4+7uvmHDhk2bNmVnZ4eFhZFIpB9++EFPT0/1MGuMvmd1NejfH86lf/lF9SigFhECKkDgU9L31NEBJ0+CPXtAjfcMFYCLmkAIaB8Cat1/loLTwwPs3AlmzgR5eVI5kpcZGRk5OTmSKSiOEEAI4AhokjwDADeufH1BYCDg8eTeodevX58/f/7s2bPv37+XS4ASEQKfMgLqlOesrKwzZ86kpaXVuwFHjgA2u6G9q/Lych6Px+fzq6qq6pVCFwgBhIDC/WO0CNJLly5VVlbm5+evXLny4/q+nh64eBGujR08CBYvlqrQx8eHy+WSyWRPT0+pLHSJEGg+Ajk5ORs2bCAQCNXV1d9++21cTSASiba2tj/99NPHp7H5NWoGpTr3q2g0GpfL1dXVlYbPzg5cugTGjAEuLiU9elAoFCMjIxwuMpk8ZsyYlkInEonKy8tNTEykG2ppRYheWxBYtGjRnj17nJyc+Hx+aWlpfn7+4sWLBw0aFBgYWFhY2LFjx3baUXXK85w5c7KysmxsbORg5+MDtm8XBgRcXrSowsxs9uzZbYH4999/r6ystLa2DgwMlNMWSlIHAhkZGeXl5Z6enmKFjeZyUVYGtm5tLjFO9+OPknoN1dXVbDbbyckJAEChUPBHKzw8/Pjx43w+vy1PWsu4UgK1OufPLJZOSIhzXl4D2+6LF+d5eU04flxUUVFUVNTqvguFwsrKShaLhbzqtBpDhRfMy8uLioq6ceNGa1yoikSgrKxlPwyT7AKJRBIKhZIpAIBZs2Zt27bNxMTk33//lcpqR5eKH5/z8/M5HI6joyOOQmJiIoVC6datmywopqZwmjx6NPj7b1CjeyNNYnbmTOWgQSFXr5p91zL9e8mKSCSSl5dXfHz80KFDJdNRXI0ICAQCDMNEIhGvbiMDl3BdXd2QkBCdhoxl4xybmUFt/zYEMplsZWX18OHDIUOGMBgMXFvT0NDQ1NSURqPhl22oXp1FFawfdufOnTNnzjCZzHPnzgEAIiIi3r9/z2azhw0bNmXKFMmO4royIhGYMwe8fg3u3wemppL5dfHKSjBsGHB2BqdPA+RZtg4VLfj/+vXrsrIyHx8fCoUCAMD9y5DJ5ICAAFdXV2V3kMlkbtmyJT8/X1dXd+3atU+ePPnrr7+IRGL37t3DwsKUtM6iAv0wgCk6lJaWTps2Da915MiRQqGQw+GMGzdOqp0FCxbk5+efPn362bP4CROwfv2w8nIpkrrL/HzM2RkLCsKEwrqkFv8XiUSFhYV8Pr/FJVEBlSCQlpa2Y8eOPXv2MJlMlTSohkYWLFig7FYV/70t+WbFMIxIJOrq6oo99GEYtmjRIgBAamrqqQunjpYeDfgQcOKE88yZemPHglu3gIGBZAU18Y4dwcOHYPhwMH8+OHIENO7sQKY0nnDu3LmcnBwajbZ48WIlvX0baBklNwsBR0fHr776qsVrY82q+xMiUsV6mEAgIJNrXxwEAuFQTXB2du5o0tGAaPAb57c/bp84cKBYRwdMmAB1SeSEDh2grMfFgS++ACKRHIKmkgoLC9k1AT8l1xT5p5LP5XLfvXsnftuqt9tImNuOv4LluaCg4M8//8zJybl+/ToAYOjQoeHh4Tt37pwwYYIsrzMCZ5wOPN2T0nNV4qp9F7devQq1PMePb0Ckra3B3btwnr1oUStE2t/f39LS0sfHh4rOY0rciePHj1+4cOHYsWMSaSjajhFQsDwDAAwMDL766iscknXr1nXu3NnT0xM/ayqFE4lE6tq16+e0z72B9/6q/XEFMTExoLoa+PsD+dqctrZQnu/fB599BgoKpGpr/LJr166LFy/28vJqnOxTy+VwOAKBgMPhfGod19r+KnuC3lD94rWBioqKf//99/d/f9f/Wf/oq6NMJjZ6NNanD1Zc3EDRkhJsyhTMwgKLjm6AAiU3F4GMjIyzZ8+mp6c3twCiawMC4me+DXU0UVS562HNeQvS6fR+/fr1A/3sze0nn5v8z3//7Nq1av36rkOGgNhYYGUlU4eZGYiKgr+5c+FAfeCAvDU0mVIoQR4C9jVBXg5Ka5cIKP57u9Uw+Dr4HvY6fD75/PRLn3+z/sWAAWDwYCB1+Opj5VOnglevQFYWtCUYF/cxXX0xoVB448aN6OhosY6E+nhBLX+iCGiQPAMArKnWC4kLORgn8PbUr7e98/ODNgITEhq4N3Z24N498OWXwM8PrF3b0JHp6upq1axpv3nz5uXLlwkJCY8ePWqA43aTXFBQEBcXx2Qy2w3HiNEaBNT/vS15IwYMGMDn88eRx11jXxvwe79jy45bWAQMHgwPUPr6ShLWxYlEsGIFGDEC2hK8execOgU1ySRCRkbGxYsXiUTinDlzcC++EpkKjpqYmFCpVAzDzM3NFVy1aqsTCASnT59mMplv375diIwoqxb8NramWfJMJBKHDBkCABgEBnl29Az5c87K4a+3266fMIG0Zw/ce5YfevQA//4LNm4EvXuDn38GYWGAUGtvOSUlhcViEQiEzMzMxuW5vLz85MmTIpEoKCjIVL7qqfzGxan29vZz5swRCARWcib9Yqr2EUEqN+3jPslwqVnyLMnejO4z3C3cJ5+fbEOPO33lVOiMzs+egd9+A3WaKZK0AOjqgi1b4CAeEgJiYsCxY/gJDy8vr6ysLAqF4u7uXr+A9NXDhw8ZDAYA4PXr18OHD5fObt51hw4dmkeocVQYhollmEwmz5gx4927d3369NE4RhFDjSKgWfNnKVY9Oni8CH3RybDTgpeeG85djIuD69nl5VJUEpejRsGzHXp6wNMT91xpbGy8aNGi+fPn6+rqStBJRx89epSUlEQgEAwMDDw8PKSztfH6w4cPDx48YLFYAIC7d+/u3Lnz5MmT4o526tRp2LBhhoaG4hQUaRcIaLQ8AwDoOvTTAacPjDuw/llot3XTeCSGtzdITm4YW3NzcPky+N//wOzZ0AhZfV8HDRXDzZJRKJQJEyY0/lneUA3NSReJRNXV1c2hbJImPj7+119/baYDRNnaqqur//jjjwcPHvzxxx8AgISEBCaTWVRUpCGKn7IMo5RmIqDp8ox3Y6rb1NeLXrOE5e9H9uw+MbZvX7B7d6NKn/Pnw92szEzg4AC++gpkZDQOh6+vb69evby9vXGbFTweLyEhobyxL4HG65OTy+Fw9tWEly9fysluYdLDhw8rKirS0tLY8vXdW1bdgAED6HS6ra1t418xLasUUasDgfYhz3Ari2791+y/VvRdccPw80G7Z+05kTFoEPjvv4Yx69IF/PUXtM6fnQ26dQOTJoGbNxt6B+jq6k6YMGHo0KH4HPL06dPR0dHHjh2TtWLRcHtN5BQWFnI4HBaLldDg/lsTNUhmu7q60mg0Y2Pj1jlV0dHRCQwMHDZsGG6AaeDAgStXrpw6dapkEyjeHhFoN/IMACgrKwP/gjBSGIlQVjjFjT10had38datDQlpze0YMABudqWmQk93ISHAzg4O13fuNLRZjd9CDocjEomENUFRN9XW1tba2trMzMxX/s5bvXYwDIuOjg4PDy8sLKyXUXcxatSoFStWzJ8/X7yIVZdT7/+bN29OnDgh1/2AlZXVkCFDaDRavQLoop0j0J7kubS0lMfj6XB15pvOfxH6wrl3Pljh+Mu/a339K7OzG7wPUOHVzg6ufufkwPXxigrogsPCAkydCiIj5U6wp0+f7unpOXHiRMnDWHw+PyYm5u7du1h9Y1QNNlw/g0gkzp49e9myZdbN8Fyfm5ubmppaWFgIT6g0EHCzHg1kwmQ+n3/r1q2MjIxWT7MbqRxlaSYCmrtfJYuXk5OTu7t7eXn5qFGj6HT6+annn+Q+WdXhm6e5XbrNX71z6pdLQuv57AYAFBQUnDlzhkAgdOjQoaioaPjw4Z5Hj8IB/flzcPUqdJc1fz7T1fVdly5WM2Z0HD8e6OsDAMzMzCZNmiTFQFxc3PPnz4lEoqWlZffu3aVyFXtpZmZGpVIJBEJbfAiTyWQqlSoQCMTWjhXLJKpNAxFQtzzn58s3BSgPKgKBIHWO2sva6/GCh7fTby+grQxLPXhgzncxP39hY/3xoyMxMRH3pMHlcnk83sOHD6EhfiIRGiLs3x9s3szLyHi8Zo3N27f0BQsAhwPc3UGvXqBnT+DhAWfdnTuLGTEyMqJQKEQi0UCOCRUxlWIi+vr6S5YsYbFYJiYtc1wu2TyBQAgNDc3Pz5f1US5J1oo4j8eLjY2l0WjiFYdWVIKKKAMBtcpzRQVwcoJKXXPmwK/fOqP5Le3nSMeRaWviTzy/uPzK1w7b9s13Wn8orHZpp0+fPikpKcSawGQyZbVKKPb274YOje/f38bGZsaAAeDFC7gwfvMm2LED5ObC4bpLF8ikk1OfLl2sbGwI9vaqURqh1oSWQiFFr6ur6+DgIJUofclmw0Pn4mBo2IDKjpgCPHjw4OXLlyQSydLS0s3N7WMGiqkbAQXb92x+d2ptHRYWgjNn4Dz23TswcSIICoJuMepUwEpLSzMyMrp3797MfRQ2nx16ZN+Z7C2Wgr6n527xdf2o3lRUVBQVFUWn02fMmCE2foRzy+PxioqKrKysxGtLJSUlVVVVDh06wONd79/X/sUjOTlAKISvns6d4STc3ByYmEj/9PQAjQZwwTA2hsqnenpQg60Vobq61lwLHsGwWn2aqiogEECzDwIBqKyELJWVwb+VlXCpj8WC3xpcbj0CFgtm4ekiEVxHaDKQybAXVCrsjokJMDaGRljNzPKrq5NKSqpMTLxmzuzo7Q1pUGgGAiqw76lueRaj8OYNOHECnD0LH9DAQDB7Nr9nz3379jGZTGtr63nz5okJm4wkZzI+2/pLhvkBH/MJe6Z93btTbwDAuXPnUlJSKBTKlClTnOuf2ZCqsLi4ODIyUiAQ+Pj4DB48WCoXEwgIxcUgPx/k54sKC0FpKbGiop5tdwYDSiCbXStmUuVpNElHDVKZAEBzqwShsAH7LAC+GoyNYSn8ZWFgACgUQKdDS8YmJvAvnV4rfvgbRJIAbxpPJxI/fg1JscRkAj6/ljE+HzCZcPRms+F7pLwcMBigtBSUlLAzMyn5+ZScHEhgYQH3+R0d4a97d9CvH/yiQUEGARXIs1q/tyU77OEBP3G3boWbSadPgxEjyJ0793dweOHiwm+hUrSrvWnawR07Dod9f/OX/ozBHpY9vxy00N3VPScnh0wmd5aYEku2L45XVVXx+Xwej1daWipOxCM3b95MTEy0sbGZPn16dnZ2VEoKkUyes3JlY+c3yssBhtWOigDAkbPOgrxU5bje5YcPH6j6+lPnzas1jqejgy/RAXFEtphiU5o3Y4fLhngoKgLp6VBjJz0d/mJiQGIitDABTVTU/ZrCvK4u9L+tCGiMPOMdIZGgv4zRo8HBg4To6D6HD/v89puwTx8oEtOmNWBxXz4EqxfYBk8M/2bj/07dO7ai4H8i/YLpblODewY3uZrl4ODg5eVVWlo6evRoqaqTk5NZLFZubq5QKExMTGQymUQiMTMzszF5xofT5glJ0uPHDBMT6Kavc2f9mpV2KQY08dLSElhagoEDP/LG4YD4ePDsGdxEwDf/XV2h5v348ZAMOUX4iJTiYx+XghVfd1tq1NcHM2fq3btHyM0lz5gBzW537gwCAkB0dL3Fm0absLQExw6YJESsHPE2ufrY1cf/Cv3PjLP/1T4sJuxOxh2BSCC3NIFAGDZs2OTJk2Ulqnfv3oaGhl27dsV96FhYWHTs2FGBzhz8/PwsLCx69+4t27RcVjU0UU8PeHnBU6snTkBV++Ji8P33UEtv/HjQsSNc+4yKgpMReQHDsJiYmJMnTyL/3vLgaTpNY+bPAGRkZDx//tzLy0u+xkVyMjRXcPo0nFtOnw6Cg+uNCU319PlzsGEDuP+IM2zebVrvKw8KrvCFfP+u/hNdJvo5+RlS0YqOHATT09OvXLliYGAQEhIitYgoh7rJJIEAeiq7dg3u/GdmQgcJs2fDd7TEl0h6evr58+erq6vd3Ny0T/9UBfNnTRmfMQy7fPny27dvL1y4IP/BcHWFtgrS06FraC4XfpO7ukKtrw8f5NPXT+3bF1y/Dh7c1qPnj/9z/uG+9/PXd73a2bDzurvrLLZZjD09dueTnS/yXwgxabeD9atp+qqiouLu3bt5eXlNk2o8RVxcXEVFRUlJSW5urgKYJZOhK7IdO+BeRkICNCX188+gc2fRwoVwyl0TTExMKBSKnp6eFtiEUABiLa9CU+SZQCBQKBQSidTE1hSRCJ+Do0dBURH48Ufw6BFcWR00CEREwKWmpkLfvnAFPSsL9OtL/CnU589l25YI3z0JeTXSceSd9DvDjg8z2WLie8J33d11V1OvZpRntEK188yZM3FxcWfPnlXgWY6muqWs/P79+9NoNBKJpHjPFc7O4Ntvq+PjL8ybl/zypahPH2g06tIlEzp9wYIFQUFB3t7eyuqVVterQd/bbDY7MzPT0dGxCZGWuh8fPsDv8CNHQGEh3MEODoZWSursDUnRSl6y2eDkSajxWVwMQkOhbyxbe0FCUcKTnCdPc5/+8+GfNEaaHkXP1dzV3dLdzcLN3QL+tTOyE29TS9YmjuOHKAwNDb/88kvFi4G4GXmRiuoKNp/N4XPKueUCkaCyupIn5LH48DXH4rF4Qh4AQCASVPGqpEobUg3JRLIOWUefom9INdQh69B16PoUfRNdk/MnzpcVlym8O+np6e/fv7e3t798+TKXy+1qbDyTxQLh4XD7bfVqaIm5ddv1Uh3TsEsVfG9rkDy3FfwXL+ACzKlTcLNkxgxozKBJ1Si43wsdY/32G1QJ8/aundDh5sO4Am5ySXJycXJiUeLb4rdJxUkZZRkUEsWGbmNjZIP/tdC3MNUzNdM3M9UzhRE9M6qQ+vr1a2dnZwsLC9keYRhWXl5uZGSEvxSkXg18EZ/JY7J4LK6AW1FdweFzWHxWZXVlZXUlk8dk8phV1VVl3DI8UsWrqqquKueWs/gsFo8lKaXGusYkAslI14hKotIo8AQVjUqjkqgAADKRLLtYUMWrEogE1YJqNp+NvwUqqys5Ag5XwAUAUAHVkGjo3NnZim5lTbe2NbK1pls7GDs4mznTdeiyfWwyhclkhoeHs1gsW1tbKpVaWlo6ceJEOzs7uO999izYtg3ucn/3HViwAO7SaVFA8tzym8lmwwl2ZCR0izN8OByu66+4NFRjfj5cazt7Fs7sRo+Gu2Njx0JFCcnAFXDTytKyK7JzKnJyK3NzKnNK2aWlnFIGh8HgMErZpfj0W5+ir0PSMdSBgx6FSDGgfnSZWVZWxuPxCASCCIi4GNfAwEAEREweU4gJK6s/LvkSAMFY15hGpdEoNAOqAR43pBoa6hga6RjRdeiGOoaGVEMalWaia2JANcApTfRMdMm6emQ9SZ7bEucIODnFOU/fPKVZ0jgkzoeqD7mVudkV2bmVuRllGWXcMlsjWzcLtx6WPdws3Ohc+rtH70xoJvPnz2/87BebzT5w4ACHw7G3tw8KCpLmEMPAhQtgwwZRVdWHefMs1qzRVb7CvDQPyrlG8twGXHNywMmTWGQkyMvDAgKIISFw4t0MX7OpqeDcOfhOSEiAquV+fvA3YECz9k0rqisYHAaTx+QJeVXVcNDDh1xxN27cuMFisSgUipAv1AE6rq6uXgO8DKgGRALRSMeIQoLCj78OxEUaj5SXlyckJLi5uSnPTFJDDBQwCxKLEpOKk5KKkhKLEuM/xHMxriXBcmTXkf7d/YfYDbGmWzdYtqAgJyfHw8NDp4ERWMjjxc6dOyAmhqCnZxwZCUaObKiqdpSO5LmtN+vQoUP6CQl93r7t9upVNZEomj7dYP58IM9sJZvNLigosLe3F0968/KggZObN6HbHQDAqFFQsH19ga0ttKyQmZnp4uLSUvMgb9++vXv3rrOz8/v374VC4axZsxrTRWmg91wuNyUlxc7OzsTEZN++fQwGg06nf/nll1Jf7w2UVlby+/fvI6MjCymFVCfqo9xHCYUJ1nTrwXaDB9sO9u/q34hsy2WIz+fv3buXU1Hhm5jodfs2/GTauROao2jPAclzm+4ehmG7du1iMpl0Oh3w+R3j4/u8e+ecmgrMzOBHeEAAnDHXjNhCoXD//v0cDsfW1nbmzJlSrQqF4J9/oGD/9Rc8f9WpEzA3T+3YMcPDo3rdugmqP4xw9OjR3NxcAwODFStWHDhwgMFgGBkZrVixAhp7aGC4E/eourr62rVrFApl3LhxJGWqalVUVzzKfvQo59H9zPv/5P7Tp3Ofz10+D3ANcDar5/BAzJhsJD09PSkpycfHx7S6GnzzDVQ1W7MGOkJpt0tlKpBn5ep7Jicn//LLLwQCYfPmzXDBQ7WBQCCMHTv28ePHgwYNunXrVrq7e2aPHk7W1gGGhqSYGDB5MjRsMHYsmDqVN2QIn8+vrq6GJo1kAokEBX3bYtQAACAASURBVN/bG26QVVWBJ09Eu3aVvH/f5e+/rbdvB/b28AyCuzt0pOXuDjfFlephOj8/v6CgAPcyKBKJZs+eHR8f36NHj2PHjjEYDBcXl88++0ymBx8TcMvERCLR2tq6d294UkVJwUjHyL+rv39XfwBAKaf0eur1qLdRG+9vtKJbfeb82VS3qT42Po1/UDjWhFr2TpyAmxDLl8PTeIcPw6kTCvIQUK48r1u37tChQ1VVVZs2bTp69Kg8BpSb5lYTAAC2traXLl1KS0tLzc5OGj/e49AhsH8/dJFz6RL44gs9gWB+v37JdnZO8vzOS7JoaAhGjya6uFi/efPGy8uEy9VNSgJv3oCkJHD7Nnj7Fmqj2trCg0a2tvBnZwd1HK2s4NKapWWzJuGSzeFxPp/P5XJxa9iXL1/m8/kkEmns2LEUCsXExGT48OE8Ho/BYLBYrPfv38sWl0zp0KGDrq4ugUBQ5XzbTM8s2DM42DO4orriWuq1y8mXx5wa09GgY7Bn8Nyec22NbCU5bDA+aBDUCd+9G4wbB5c5d+yQVCxrsNQnlqHc/apRo0bF1sw+xREMw7Zt2wYAuH//fiPGsZRxF96+fXv9+nXcl1U9F1NCIXj6FB4M+usv8PIlNGAwfDgYMqT23F8zltDE3MbFPUpMrLS2Hpafr5eVBQ2WZWbCffG8PHiskEiEx6XNzGqPEtPp8HSjkRHclMHPNeLruPjxR3GdPB7v9u3bQqGwW7duzs7Ojx8/Li4uplAoY8aMqaoi4bbMBALw7FlKSUmJg4ODvn6ti138WHSNITHYujgUFsIF9oaWoMUnR/Czz/gBTQpFJBCU29rSzc3JxsaQZxMTOO9o9cIzR8C5+u7qsfhjd9LvjHQcuXzA8jFdxhAJzdNu+u8/KM8MBtyb7NdP3K/GI+Xl5adPnwYABAUFwfmXOkK7/94Wgyb+siIQCLheblJSkjhXNRE3NzcrKysKhSJ92oFEAj4+8PfTT/Akc1wcHLd374YL3Do60PZQr15wpbtXL+DmJra1IMvzhw8fHj/+m8vlmphUhIYGShGw2VCwa05M1x4lrqqCkcpKqNDCYsHNV1zqxHKI18DjAQajN4ZhcXFUMzNAxAbocwrMQHnsk6eddRmGgjJDPoMuqphCKNcXVOrGVxmDciqfRRZW64uYJBFft7qCgIkk9WtIIj5GImMAuvji69FFBBJP31igQ6vWN+HSzCp4Zhw9Uw7NnEm1KOJ0LDOwef+B/jopk8cjCwQsQ0Ob0lIhgyFkMqlCITRzYGMDOnQA1ta1f+3toZkmJ6cmJh16ZL1p7tOmuU/LqcyJeBEREh1ipGO0rP+yub3mym6PSyEJunaF92jnTqg9unw52LwZaqE0FV69elVSUgIAePPmzaBBg5oib6/5yv3eNjU1LSgoqKiosLe3FyPk6OgIoMOpFtjr4PP5DY0n4mqbE2naMp6JCZgwAf4AgAeVExOh+aGXL8GhQ9CTjkgE58dOTrVn921s4KkvfMzV1cVVI6lUqlyjX/r6UL2lMQ0XJrOezQAGA5SU4L/8xEQCg2EOADmjxqIAAPBFY2YGyCbwDCluOQSOm1a1hkQkjRkYGUnv0lGp0GiESASrqaiAkbIyeEIbt1UA/+Yws7IEHz7Q2WxiaSlGIlUaGpaZmJRbWHhOnXr9/fsMKlVgYxM4e1VxMTE3FxQUQCX6ggK45Z+RAY0jCwRwouHsDH/dusE3Ya9e8pexbOg2m4dvXjdk3bnEc3v/2bvx/sYl/ZasGLjCQr/+1r/U3SWT4QrZiBFQAejBAzhQd+mCk1RXV589e5bH4wUGBkqOw+7u7q9evSIQCC4uLlKVadOlcr+309PTf/75ZyKR+OOPP3bq1EkSuGZ+e4hEooiICCaT2b9/f9z1pGQlKo0LhSAlBU6R09Jqz+7n5EBDJVU16pMGBsDMTGBqytfX17O0hMO4gYGQSIT2CajUjh071rKKm/vBLX6w2XBcFg/TgprzmzQaFNEasz7wTYG/LMzMaiPm5rVGjpRpN5vP5+/Zs4fFYpmami4PDU26eTPh2jXDkpK+dHqHioryf/4xLC7GCARy797wc7dvX/jXxUW8NiASwcORqanw9+4d/D1/Dj89evSotcLYrx98K8pdXL+TcWfr31sf5zxe1n/Zau/V5vpNed5ls8GqVbWLZNOmAQBevHhx/fp1DMP69+8/duxYyScE18YXfypKZqkm3sxnvi3MKHd8dnR0PHLkSFv4q6ioYDKZuFsJNcsziQTXr2X9VPJ4cBStscJDLi4ml5dDERUIAJOZmZJSwGSSSCSdjh1rB+3OneE4hVvkMjCAKzoGBnCApdOhISFj4+Z8OrYFz+aUFRv6heObjo77xIn2I0dSqVT8E4nCYr18/doZAKO0NLhAdegQWLwYvr9w8R4yhDhsmL093d4e7hnjAcOgFbZ//4Xkhw+DJUsgef/+8Ht5xAioq1NnMA74Ovj6Ovg+yX2y/t56xz2OYQPClvVe1tG47m0oy72+Pjh4EOoGzJsHD+fs2GFrawu17kQi2XFYjZIsy7iSUpQ7PjfCdDPfVRiGRUVF5eXljR49ut2Zkvz777/j4uJIJFJQUJDU50kjyGhCFofDycvLs7e3b9YeNZ8PJya4vN67B0fnAQOgRtfIkfCMulhY6zrG58OlicePwb178BNdKISSP24c3Dq0tKwjAuBe+r0FZxcUCAuWuCz539T/kQikj3mysffvwZQpcLpx/jzPwkIoFLZU1Ue2SoWnNPOZb1O7+E6m6v8uWLBA9Y0qsEUOh8NgMBqvUCQS/ffff3l5eY2TaVtuWhp26BA2dSpmZoYZGvLHjhUdOYI1gJVAgD19iq1bh/XqhZFImLc3tnMnlpUFISkoKNi2bduMjTPMN5v3DO/5IPNBE0CxWFhwMNahA/bwYROUaspWwTPf2A5BQUFBm14V2lu4qqrqwIEDv//++z///NNILwkEgpOTU/samRvpTnOzHB2h7sf586Co6MnPPz/BsNKNG+Eu/GefwQNw9e0Ek0hwLN+8Ga45ZmVBT0TXrsG1raFDQXS0paVlNy9Tr4eBDwNcA/xP+8+4OCO3smHLCvr68BzOt99Cvdzw8OZyq110jclzbGzsggULFOs2VTvQKykp4fP5bDY7LS1NO3rUol7gpz6bNvZAJD4H4O7AgcfDwtivXsFZbkQE3NcaPx4KtowJMSsrsHQp3CvMzgaffw4OHyYsXDjh0aPlhbmuPwz5IXlZMoZhbr+57f93vwiDi/Pyw4oV8JWwbh2sC19ilE+nnamkjRs3NtQzT09POzu7ZcuWFRcXv3nzJjk52cPDoyHilqZfvXp1/PjxLS3VIvq3b99evnyZQqG0yKMFhmFnz56NjY2lUCgNGfc1NjYuLi4mEonjx4+X3s1uEYuaQcxkMslkcvOXi44fP/7o0aOMjIwmnwdDQ8P8/HxnZ2d3Hx84l543D6rZlpVBwf7hB7gCbmkJ1ejqB0NDSBsaClXs//sPqmyfPAmIfHJHRrkNxTIyN/JyyuXhDsNNdKEzoOLi4piYGDKZLNZ441lZRWFY58hIwvXr5IAA+Rtl9VtUzZUKnvnGxmeoeVtaKhKJTE1NTUxMmt68VQ0qzW7l5s2b+fn5uIJaswuBysrK/Pz8qqqqJ0+eNFSKQCB8/vnnoaGh4meoIcrG07OyssLDw+/evds4mVJzr1+/Hh4eHhER0fR4W8MHhmG4bmlRUVGTjLm5uYWFhY0bN+4jpasrtMyYlARtAxoYwHUwDw8o3vIc07u5gV274Ob2ypXg6FHRhg1fvDw/f1OHE24Wbp4HPY+8glsn58+fT0pKunLlCr/ODUBmZmY6ABEhIfDT0tsb6ui1OeTl5R05cuTx48dtrkm5FTQmz+Hh4Tdv3rx06VJQUNDUqVOVPZwqtqMlJSUYhuno6EgqFTSnCTqdbmZmRqPRlHpcAefk2rVrhYWFz549u3XrFpcLjYGoPqSnp7NYLCaTyeFwmtM6gUDw8fExMzMb2sZDEb16QbswHz6AZcvA3r1Q0WztWugzTCbo60NrUE+e8JcuvSwQGC+cO7TqzKGdA6PW31s/9vRYvi6fTCaTagJe1MbGxsDAgGJuzjx3DgweDI0Hv3ghU2vLEqKjo3Nzcx8/fqzhhoQb23+eOXNmS4WhZSApk/rMmTNMJtPAwGDu3LktaodAIISEhLSoSKuJbWxs2Gw2h8N5+vRpUVHR7NmzW11VqwuOHDkyNjbW2dm5+ROHgTWh1S3WuAlhlZaW2tjYEGg0+GG9YAGcN+/ZA3XvZs4EX38NFVTqB1NT0127pgMAdXl++QUsHztmfGB8xeCFvzB+2TRw06x+s8QH1/X09MLCwkQiEUwJD4fKK8OHQ52TRk+e1W9N+srS0pLJZOK2R6XzNOm6sfG5/QozAAC/u+SaoEmA1+NlwoQJU6ZM0dPTIxAI4sexHoXyL1xdXcPCwvz8/JTfVG0LXC43IiLi7NmzV69erU0iEKCliCtX4DK3SAR15mfPhppl8oKjI/j9d6inZ0i0eLj0klPm1jVPv/npn5+kHCR8xHPFCmgQNjAQap60NkyePHnmzJkLFy5UgB3y1vLQnHKNyXNzymssTXBw8OjRo+fMmaOxHOKMOTg4TJs2zdfXd/LkyRrOqqLYY7FYAoGAy+UWFhZK1+nmBo4fh8JKpQJPT3iOKj1dmqbm2tERCmlSEnBmzRWE/3Mk7vrQIyMLWTIV4oWnTIFG/L/7Dk7dWxUIBIK1tXWLDh20qp22FtJaeabT6V5eXsa4+6i2oqTc8nZ2dt7e3k2aFlEuEyqs3czMzMvLy9HRcdKkSfKbFQsrhkEF26+/lt3cwgt26QLF/8VN1z4v/3n+wLLbzj6Psp7Kr3P4cPDwIRzZFy2CKmlaGrRWntV7v/Ly8irqK06olx9Na33QoEFBQUFyTRp/ZLVLF7hP9egRtPbk6Ahn1w3Iobs7uHXN4GLgOerLL4cc8f3+/ImPlUjGevSAWqb378Nvb0kX9pI07TyO5FnxN/DRo0enTp06fPgwg8FQfO2N1igQCM6ePXv06NFKGW2NRstpcGbv3lACDxyAJ9L79oUnn2VCdnb2wYMHdXRu555fHax74X/xYR4r1xaXyNM5sbODNWRkwLUxSSMPMnW20wQkz4q/cfn5+RwORyAQqF61Ljk5OT09PScn58GDB4rvmLpqJBCgPfTkZKhf4ucHF8Prm3m7fv16UVFRfHw8m11+7Luxt6Y/SdONslk5/WyU9Bbgy5cvL8XFVf35JxzqR42Sqkdd/VNgu1ouzyKR6OnTp69evWqmsoRCkB0zZoyTk1PPnj0dGjNfoJCmpCvp1KmTbo1lhS515/ulKdrvtZ4eVCl78wbqh/ToAY1D1QVbW1t9fX0dHR3cufdIT9fMdU+t3XKCY0dPDCyrsUoCSRkMxp07dxISEi7GxkLzUh07wt1prfAcWIcEaGz/WUzUfiOPHz9+8OABiUTS09OTPRCrpH4ZGhrOmjVLSZU3Xq25ufnixYsFAkG73mtspI88G5s/Q0PtHjzoP3Uq9Gq0cycwMBg3blz//v2NjY3Fm0kW+havV935/HTgvXeDunv/dfI361GjgI6ODpFIpFAoEBwdHeiGet48eAI7NhZqs2hF0PLxWbwJKY5oxV1rrBP6+vraKswAgOfPn6ekpPzVocPzI0eguRgPD3xGbWFhIWWRikah3Qi+HDCgn2jOkElz01esAGQybe7cuZMnT65dVyeTwbFjcIgePBiqiWtF0HJ59vLyGjt27Pjx452dm2vGXStuq9Z2Ap9Q6OrqmvTuDe7e5cyaJfD1fernVyzvbC+ZSD428disvhP1l/vcik8cOBAUF5t269bt48udRIKq45MmwfOZb99qAWpa/r1NIBBUoIatBc9Be+mCg4PDvHnzAAD4SZh4f/83xcVTLlwgjB0LZ9SS9k1qukQgEHaP2U3Xoe8nDPUrudm/f7+ICDAdqo3WBQIB/PorNGwyYgTUOXVzq8tol/+1fHxul/cEMd0oAmY1ASdxd3fnd+9+fs0aAxsbuJv1/LncopuGbfpy4JcxZmO+P/DvwoXQgqD0yeiff4Zq5MOHQ8NJ7Tlo+fjcnm8N4r1pBOh0+rJlyyAdhoEtW6CFwQMHoJaoTPhhyA/6FP2fHo45EhO7/ou+r15BAyrmkuZDf/wRFvL1BXfuQA9G7TMgeW6f9w1xLYUAgQAtDfXqBXW/3r2DrhEkfQjUEK/yWiUUCRc8HH3t6r3tqzwHDgTXr0Pz4B/Djz/CUu1ZpJE8f7ybKNbuEfDzg2YSxo+HpziOH4ebUvXD1z5fs/nsSRdH3Tt0P+qAm5cX9Fo5fLgE0aZN8KLdijSaP0vcSxTVAgS6d4feyP77D5r/xV0d1O/UxmEb5/Sc43d6zNyvsrZvh/ZRpG0HbtoEz2z4+rbHuTSS5/p3G121NwRYLFZ+fn49rjt0gCrfBAJcshZrh0lQbBu5zb+r/9hTYz+fybhyBX6nr1iB+/+pI2q3Io3kue4Wov/tEIGqqqpDhw6dOnVKWl/dwADcuAHd5A0dCj1r1Q8EAuHAuAMu5i7+p/29h7Lj4sCff0InH7izzlra9inSSJ7r32p01a4QKCsrEwgEbDY7V9bwGK7R2bMndNNRXCzVLRKBdHryaRKRNOPiDFd34cOHUOkzNLQBkU5IkCqusZdInjX21iDGmkbAxsbGw8PDzs7O399fDjWZDA199+gBRbq0VIpAj6z3Z+Cf70reLbuxzNYWfqHfuQMWLmzfIo3kWeouo8v2hACBQPDz8wsJCZHroxf2hESCIu3gAEVa5ji6ub75jVk3Lidf3vF4h60t9KcVGwu++qo+Aps2wW9xX1/odEvjg4LlubKy8ubNm7dv38Y7LhQKT58+feHCBVUeV9R4zBGDqkWAQoG6I507A39/2RVvRxPHqzOvbnqw6XLKZTs7KM8XLsgT6YUL4RtB43W8FSzP7969S05OPnz4MH7HNtSYXyssLPz1119Vew9RawgBCQSoVLjRbGAA/ejweBIZMNqvc78jE47MuTznXek7Jyc4Sp8/Dy341wubN4MvvoAi/f59vXRNu1C4q73S0tJp06bh1fr6+mIYJhAIRo8eLW6IURPmzp0rTkERhIAqEKiogG4sZ8zARCLZ5pZcX+Jx0IPNZ2MY9u4d1rkztmqVDNWXX2K2trX+L2Uym0xQgX/JNumHHT58OCUlBX9DEYnE7du3y31bkUgkYZ0lNwzDtm7dCgDIzs6WS4wSEQLKQoBOhxqeAweCjRsBrgcm0dKuMbsGHR301c2vwj8Ld3aGR62GD4d72PUe6l27AIsFR+m4OOhYT/NCm+R5+vTpvLqvF7nezIhEokgkqq6uFvteIBAIW7ZsAQCEhoZqHhqII21HoFMnaGnIxwc64ggKkuytDknn4rSLvQ/19rbxDvYM7tatVqRpNCj+tYFAgNpkM2dC5bP79wGdXpehKf/bNH+m0+nmdQE/j5qRkbFnz56UlJTwGiW6oKCgFStWLF26dMGCBZrSY8SH9iLAZDLL6psKlOwrj8eD67JubuDUKajRKeO729bI9vik48tuLHtbDG0buLiAW7egjy385FVtVUQitCLcoQOYOBGoyeWYZKek401+9LeIgMVipdWErKwsvGBmZmZubq5sJSqYS8g2ilK0GIGioqIdO3Zs27YtISFBtpuPHz/evn37/v37+Xw+zN2yBc6Ei4tlKVffWt1tX7fK6ko86/VrzMwM27y5PiGLhXl7YxMmYHht9TMbulLBM9+m8Vn63QCAvr6+Y02wrXPqa2dnZ2VlJUuJUhACikWgoKCAw+Gw2eysrCzZmt+8ecNisdhsdimuWPL116BPH3i4sm5lR1zkf77/s6RZLry6EE/x8AC3b0MTJj//LCaBDzr0tvX+PbQoWE9NVIJGHVEFy7M6uoDaRAhABNzc3Lp162Zvby/Xke3IkSONjY1tbGwscZtEBAL0f5WRAX1V1g9kIvn81PP3Mu8dflm77dqzJ7RltGNHfYd2Zmbwc/zhQ+gWS3NCQ98Gyk5XwbeHsruA6m/3CMTHYzQa9tdfsh25k36H9jPtRd4LcdbDh5i+PhYdLU6oiaSmYpaW2Pbt9VPlX6ngmUfjs+a8WhEnKkfA0xMOu0FB0K18/TDCYcQq71XTL0yvqK7AcwYPBvv3c2bNEj19Wv2RtmtXcO0aXDE7evRjovpiSJ7Vhz1qWRMQWLQI+tCZMkVWb2zD0A2OJo7Bl4NxbWUMwyorIwYOfDB2rKDeEcx+/UB0NFi+HFy6pPYOIXlW+y1ADKgbgQMHQHk5dLVRPxAJxJOfn3yR92Lfv/tqLA5CTcdBgx46On6YNg3w+RLUI0bA8XnOHOgNU60BybNa4UeNawICNBo4fBiuX2dkSLFjSbOMmhb17Z1vH+c8JhKJU6ZM6dnT89IlYyYTrF5dn3b6dLi0NnEiSE2tn6HSKyTPKoUbNaahCAwaBD+5ly6VZc/L2uuHIT8EXggsYZfY2dlNmjTJzs7y/Hl4CjMysj758uXQVLCfHygsrJ+huiskz6rDGrWkYgTy8vLOnj2bnJzcrHZ37gTPnkHLQzLhG59v+nTuMyd6jgir9Snt5ASVxJYsAS9f1qfesQP07g0NjLJY9TNUdIXkWUVAo2ZUj8DFixdTU1OvX78ukHaHIY8XMzNotTssTNbPO4FAODLhSGJRIj6Rxgt/9hk8Uzl5cn3DJ0QiVCalUORqqshrVcFpSJ4VDCiqTnMQoNFolJpAIpGaxdWCBcDKCkq1TDDVMz028di3t7/FVbvx/E2bgKsrCAmpbxtUVxcO8u/egTVrZKpRfoL8nW/lp6pgb135nUAtaDQCPB7v3bt3LBarBVy+eYPp6mKvX8stsuzGMq/fvQQigTiXwcC6dME2bRIn1EVSU6Ha94EDddfwvwqeeTQ+K/+ViVpQEwIUCsXZ2Vl8VrdZXPToAc18LlsmVyt7y8gtRayi3U92i6syMYG7ztu3w4PV9ULXrjAjM7NeovIvkDwrH2PUQvtC4KefoLuckydluaZRaL9P+H3D/Q1pZWniXA8PeCY6KAikfUyryRwyBNSY7hBTqiCC5FkFIKMmNA6B0tJSfj2NEAkODQ2hEujKlfVXumoJhtkPm9lj5pLrSyQKgFmzwIwZcG2MzZZMVkMcybMaQEdNqheBmJiYo0ePHjx4sMF178BA0L8/WLdOLp9bR259lf/qwtsLkrm7dwM9PRnDoJIUKokjeVYJzKgRTUIgMzOTzWbzeLwqeQ7rajnduxeqjDx5Isu4qZ7p1lFbw2LCxEc1AABUKjhzBvzxB4iKqi2RkJBw9+5dsUEu2XqUkYLkWRmoojo1GoFx48ZZWlr26NGjQSv8AEADY6tXQ7NE8vauQzxDupl323S/xrlsXV8dHMC+fVDJJC8P5OXlxcTEPHr06MaNG3X5qviP5FkVKKM2NAoBW1vbxYsXjxkzpgmuvvsOWgg7eFCWjEAg7PffH/48/FXBK8nc4GAwahTckSaRoKVNAoFApVIlCZQdR/KsbIRR/e0WAR0dOOCuWyd7OhoA4G7hvnzA8oVXF4qVQPF+HjwIT2ScOWM5bdo0f3//pt8aCoUHybNC4USVaRkCo0eD0aMlz1KJRKKioiLcnvyGoRtK2CW/v/xdstNGRnDevW4dqKiw7927d3NV0ySraEMcyXMbwENFPwUE9u6FJrtjYvC+njx5MjIy8siRIwAAfYr+br/da2+vLWIVSSIxZAh0ED9njqyJBEkqpcSRPCsFVlSp9iDQqRNYvx4KaI217dLSUjabXVVVhRstmdht4mC7wV/Hfi3VX9wEv4wTDikqxV8ieVY8pqhGbUMgLAzQaLjnm9GjR1tYWAwbNkzsEGbf2H0Xky/ezbgr2WsqFZ6ONjOTTFNFvE3+blTBIGoDIaB2BMhk6CZj1Cgwe3b3miDJka2R7feDv198ffGbxW90SDriLE9P4OkpvlJRBI3PKgIaNdO+EfD2BgEBoMb/sWxHVnuv1iXrrr+xvqKi1hioLI1qUpA8qwZn1IrmIpCXlxceHh5Tt+LVIKObN0PlrzdvZAnIRPJs09m/vvx1W8Q2JpMpS6CyFCTPKoMaNaShCFy9erWwsDAxMbG4uLgxFu3toaZIA2tcHbgdXIHrteprSJ4bwxDlIQSUjYCDgwONRqNSqUZGRk20tW4d3Lh68UKWbOLEiaFdQlOx1Pe897K5KktR/HoYn88nkUhEYu3ILxQKiUSieDFQZR1DDSEEmonA6NGje/fuTafTm9bNtLKC1g42bgRXr0pVbmRktGT2kvx7+atvrX4y/4m6HngFf2/v3r37iy++mDRp0tmzZwEAd+/enT59ekBAwLNnz6T6jy4RApqDgLm5edPCjLO7di109C5t17O2K9/4fJNdkR31tu6Mlep7KGnfqO1xkUiEYRiXyx01ahSGYf7+/hwOp7S0dOrUqVKVq8CWklSL6BIhoBgElizBAgMbqiriRYTDrw5cAVeWQAXPfJu+t/Py8th1FhkIBEKXLl3wz4zIyMjPPvsMAFBdXa1bE8rKyvBXFYZhixYtAgCkqtWNgOrfm6hF7UFg9Wrg4gLNC3XpItupeb3m7ftn395/9q7xVoN9zzbJ84MHD9LqjCYRicTvahzhXrlyJSEhYe/evfh5MbzD4ukEgUA4dOgQACA0NFQWC5SCEGgHCDg4gEmToIv3fdCvlVQgEUi/+v06+fzkuT3nmuubS+Uq+7JN8jxjxgwp/mJiYv744499+/ZVVVXR6XR7e/v4+HgGg9G7d28pSnSJEGjHCKxZA4YOhXrdFhayvRjhMGKA1YDNDzfv8dsjm6vUFAWvh5WVldna2m7fvv3YsWMAgJ07d0ZFRcXFxW1oQLFGqX1DlSMElIVA375g4ECoBNpA2D56e8SLi+u6swAABbBJREFUCEkzoA0QKjiZgB8TUXCtzaguNDQ0IiKiGYSIBCGgkQjcvAntemZlAQMDufwFXQ7CMOxUwClxrgqeeQWPz2LWUQQhoOUI+PkBOztQ8x0qt6ebh2+e0UN6QiqXUoGJSJ4VCCaq6hNDYNUqsGuXXIOBAAB7Y/txXcepGBEkzyoGHDWnRQhMnw7d4ogt9GpAz5A8a8BNQCy0UwTIZGhBf+tWWWdX2dnZRUX1jBCppotInlWDM2pFSxFYsABa/7x9W7J7z549O3v2bGRkZF5enmS6CuJInlUAMmpCexHQ1weLF4Nt2yR7WFJSwuVyeTxeeXm5ZLoK4m3SJ1EBf6gJhICmI7BsGfRf9+IF6NMHZ3X48OFcLtfAwMDV1VXFzCN5VjHgqDmtQ8DSEto52LED1JwpBADo6up+/vnnauknkme1wI4a1S4E1qxp5ISGKruK5s+qRBu1paUIiE9oqLt/SJ7VfQdQ+9qBwJo14OhR0LgFMuX3FMmz8jFGLXwKCPTtC13AHz6s3r4ieVYv/qh1LUJgyRJw6BAQCtXYJbWth6Wlpe2TdxxcjVg0v2kWi0Wj0ZpPr1GUiHll3Q6hECqKrVol124JACAlJUVZTdfVqzZ5ptPpw4YNq2Ojnf3/8ccf169f386YrmMXMV+HhBL++/o2Uunt+mpkjVC2Oktt8mxpadmjR49W863egqampoh5tdyCdo18x44dlQ2a2ubPKvZbr1gcEfOKxbP5tSHkG8dKbfZJGmcL5SIEEAKtQEBt43MreEVFEAIIgcYRUL88f/jwYdmyZY1zqYG5fD5/7dq1wcHBO3bsUJcNtlbAcvXq1dDQ0MjIyFaUVW+R3NzcpUuXzpkz5+LFi+rlpNWtx8bGHjx4sNXFm1VQ1oq/ilPCwsL69++v4kbb3lx1dXVKSgqGYStXrrx161bbK1RBDcXFxePHjxeJRF988UViYqIKWlRgE/n5+YWFhXw+39fXl8FgKLBm1VRVWlq6YMGCSZMmKbU5NY/Pp06dGjlypKGhYbPePZpERKVSu3XrBgCoqKjo0KGDJrHWIC8vX74cPHgwgUAYNWrUkydPGqTTyIyOHTtaWloCADAM09PT00geG2Nq8+bN33zzTWMUishT3X5VSUnJ6tWrxTz7+vr6+fndu3dvR02oqqrSZKnOz8//9ttvxcz7+fkFBgYCAA4fPuzg4ODh4SHO0uQIj8ejUCgAAAqFwuPxNJlVubxhGLZixYpVq1bp6urKJdDYxJs3bzo4OFCpVB6Px+Vylce/6uTZ3Nz8+PHjkoinpqaSyeS1a9empaUdPnx45cqVkrkaFe/UqZMU8wCAyMjIrKysn376SaNYbYQZV1fXK1euAACSkpJ8fHwaodTALHxqM2LECH9/fw1kr3GWuFxuUlJSfHz827dv79+/7+fn1zh9q3M1Yr8qKCjo5MmTre6DWgqWlZUNHjwYl4rg4OD2Ih7r1q0rLi4mkUj79+8X++hWC4AtbTQuLm7NmjWenp4AgA0bNnTu3LmlNaidnsVirVq1Kjw8XHmcaIQ8K697qGaEwCeFgJrXwz4prFFnEQLKRgDJs7IRRvUjBFSHAJJn1WGNWkIIKBsBJM/KRliD6k9NTc3OzgYAcDicx48faxBniBUFIYDkWUFAtodqTE1NFy1axOfz161bx+fz2wPLiMeWIYDkuWV4tWtqc3PzefPmzZkzRygUDh06tF33BTEvFwG0XyUXFq1NLC4u7tq1a2xsbL9+/bS2k59wx9D4/Gnd/K+++urKlSs//vijUK1m6z4t0FXYWyTPKgRb3U2dO3fOw8NjyJAhs2fP3r17t7rZQe0rHgH0va14TDW2xtTU1C5dupBIJABAcnKy6r2laSwyWsMYkmetuZWoIwgBgL630UOAENAeBJA8a8+9RD1BCCB5Rs8AQkB7EEDyrD33EvUEIfB/t3egE21QkYIAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if your data is more complex than a straight line (or an (N-1)-dimensional plane in an N-dimensional space)? Surprisingly, you can use a linear model to fit nonlinear data. This can be done with Polynomial Regression.\n",
    "\n",
    "Polynomial regression relies on polynomial features, which can get features’ high-order and interaction terms. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "For instance if you have the features $X$ $Y$, polynomial features up to degree 2 will be $1$, $X$, $Y$, $X^2$, $XY$ $Y^2$.\n",
    "\n",
    "Polynomial features up to degree 3 will be $1$, $X$, $Y$, $X^2$, $XY$ $Y^2$, $X^3$, $X^2Y$, $XY^2$, $Y^3$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "housing_poly = poly_features.fit_transform(housing_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "230/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_poly, housing_labels)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = lin_reg.predict(\n",
    "    poly_features.transform(housing_prepared)\n",
    ")\n",
    "poly_mse = mean_squared_error(\n",
    "    housing_labels, housing_predictions\n",
    ")\n",
    "poly_rmse = np.sqrt(poly_mse)\n",
    "poly_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see how the RMSE is with respect to the min-max range of prices (\"labels\") in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lower housing price {}'.format(housing_labels.min()))\n",
    "print('Higher housing price {}'.format(housing_labels.max()))\n",
    "poly_rmse / (housing_labels.max() - housing_labels.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see how the RMSE is with respect to the min-max range of prices (\"labels\") in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Q1 = {}'.format(housing_labels.quantile(0.25)))\n",
    "print('Q3 = {}'.format(housing_labels.quantile(0.75)))\n",
    "poly_rmse / (housing_labels.quantile(0.75) - housing_labels.quantile(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing_num.iloc[:10]\n",
    "some_labels = housing_labels.iloc[:10]\n",
    "preprocessed_some_data = num_pipeline.transform(\n",
    "    transform_housing_rows(some_data)\n",
    ")\n",
    "print(\"Predictions:\", lin_reg.predict(\n",
    "    poly_features.transform(preprocessed_some_data)\n",
    "))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding many polynomial features can increase the complexity of your model, which can cause overfitting.\n",
    "\n",
    "To solve overfitting you will have to rely on regularization techniques, such as Ridge or Lasso Regularization (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Using Cross Validation\n",
    "\n",
    "Let's try and use the function `sklearn.model_selection.cross_val_score()` to evaluate our Polynomial Regression model on the training set using a 10-fold cross-validation with RMSE as a score. Print out the mean value and the standard deviation of the RMSE across the 10 validation iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "start = time.time()\n",
    "scores = cross_val_score(\n",
    "    lin_reg, # model we want to train\n",
    "    housing_poly, # features\n",
    "    housing_labels, # labels\n",
    "    scoring='neg_mean_squared_error', # -MSE\n",
    "    cv=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_rmse_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_rmse_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this function to print out mean value and std of the scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", \"{0:.2f}\".format(scores.mean()))\n",
    "    print(\"Standard deviation:\", \"{0:.2f}\".format(scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(poly_rmse_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to try more scoring metrics at once while performing cross-validation, and I want to get more information than just the scores themselves I can use `sklearn.model_selection.cross_validate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "start = time.time()\n",
    "n_splits = 10\n",
    "k_fold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "cv_res = cross_validate(\n",
    "    lin_reg,\n",
    "    housing_poly,\n",
    "    housing_labels,\n",
    "    scoring=['neg_mean_squared_error', 'r2'],\n",
    "    cv=k_fold\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Duration: {} s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_rmse_scores = np.sqrt(-cv_res['test_neg_mean_squared_error'])\n",
    "display_scores(poly_rmse_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 7:</b> Use the function `sklearn.model_selection.cross_val_score()` to evaluate our Stochastic Gradienr Regressor model on the training set using a 5-fold cross-validation using the mean absolute error (MAE) as a score. Print out the mean value and the standard deviation of the mean absolute error across the 5 validation iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Ridge Regression\n",
    "\n",
    "Ridge Regression is a regularized version of the Linear Regression algorithm: we add to the cost function a regularization term. This term for Ridge is equal to $ \\alpha \\sum_{i=1}^{N}{\\theta_i^2} $. This means that it forces the learning algorithm to fit not only the data but also keep the model weights as small as possible. An important thing to note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized performance measure to evaluate the model’s performance.\n",
    "\n",
    "Ridge regression is a type of $\\ell_2$ regularisation, because the regularisation term is equal to half the square of the $\\ell_2$ norm of the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=100000, solver=\"cholesky\")\n",
    "cv_res = cross_validate(\n",
    "    ridge_reg,\n",
    "    housing_poly,\n",
    "    housing_labels,\n",
    "    scoring=['neg_mean_squared_error', 'r2'],\n",
    "    cv=k_fold\n",
    ")\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_rmse_scores = np.sqrt(-cv_res['test_neg_mean_squared_error'])\n",
    "display_scores(ridge_rmse_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6  Lasso Regression [skip]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression is a regularized version of the Linear Regression algorithm: we add to the cost function a regularization term. This term for Lasso is equal to $ \\alpha \\sum_{i=1}^{N}{\\mid \\theta_i \\mid} $.\n",
    "\n",
    "\n",
    "Lasso regression is a type of $\\ell_1$ regularisation, because the regularisation term is equal to the $\\ell_1$ norm of the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Elastic Net [skip]\n",
    "\n",
    "ElasticNet combines $\\ell_1$ and $\\ell_2$ regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "el_net = ElasticNet(alpha=100000, l1_ratio=0.01, max_iter=10000)\n",
    "cv_res = cross_validate(\n",
    "    el_net,\n",
    "    housing_poly,\n",
    "    housing_labels,\n",
    "    scoring=['neg_mean_squared_error', 'r2'],\n",
    "    cv=k_fold\n",
    ")\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elnet_rmse_scores = np.sqrt(-cv_res['test_neg_mean_squared_error'])\n",
    "display_scores(elnet_rmse_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Decision Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtree = DecisionTreeRegressor()\n",
    "cv_res = cross_validate(\n",
    "    dtree,\n",
    "    housing_num,\n",
    "    housing_labels,\n",
    "    scoring=['neg_mean_squared_error', 'r2'],\n",
    "    cv=k_fold\n",
    ")\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_rmse_scores = np.sqrt(-cv_res['test_neg_mean_squared_error'])\n",
    "display_scores(dtree_rmse_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_5 = DecisionTreeRegressor(max_depth=2)\n",
    "most_important_feats =['sqft_living', 'grade', 'sqft_living15']\n",
    "dtree_5.fit(housing_num[most_important_feats], housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "os.makedirs(os.path.abspath(os.path.join(\"..\", \"output\")), exist_ok=True)\n",
    "export_graphviz(\n",
    "    dtree_5,\n",
    "    out_file=os.path.abspath(os.path.join(\"..\", \"output\", \"kc-dtree.dot\")),\n",
    "    feature_names=most_important_feats,\n",
    "    # class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "res = plot_tree(\n",
    "    dtree_5, \n",
    "    feature_names=most_important_feats, \n",
    "    rounded=True, \n",
    "    filled=True, \n",
    "    ax=ax,\n",
    "    fontsize=18\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_regr = RandomForestRegressor(n_estimators=10) # new default numnber of estimators is 100 since 0.22\n",
    "start = time.time()\n",
    "cv_res = cross_validate(\n",
    "    forest_regr,\n",
    "    housing_num,\n",
    "    housing_labels,\n",
    "    scoring=['neg_mean_squared_error', 'r2'],\n",
    "    cv=k_fold\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Duration: {} s\".format(end - start))\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rmse_scores = np.sqrt(-cv_res['test_neg_mean_squared_error'])\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'n_estimators': [3, 10, 30], \n",
    "        'max_features': [2, 4, 6, 8]\n",
    "    },\n",
    "    {\n",
    "        'bootstrap': [False], \n",
    "        'n_estimators': [3, 10], \n",
    "        'max_features': [2, 3, 4]\n",
    "    },\n",
    "]\n",
    "\n",
    "forest_regr = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(\n",
    "    forest_regr, param_grid, cv=5,\n",
    "    scoring='neg_mean_squared_error', return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = grid_search.cv_results_\n",
    "for mean_score, params in zip(\n",
    "    cv_res['mean_test_score'], \n",
    "    cv_res['params']\n",
    "):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search\n",
    "\n",
    "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. `RandomizedSearchCV` implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "* A budget can be chosen independent of the number of parameters and possible values.\n",
    "* Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Ensemble Methods\n",
    "\n",
    "A possible way to fine tune your model is just to combine the models that perform best. You can get a good ensemble estimator out of a bunch of sloppy estimators. Random Forests are just an ensemble of Decision Trees trained on a random subset of the training set. Check the `sklearn.ensemble` module for more Ensemble Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Your System on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying various your models and hyperparameter combinations you will eventually have an algorithm that performs sufficiently well. At this point the time comes to evaluate the final model on the test set. \n",
    "\n",
    "<b>Exercise 8:</b> Evaluate your best model on the test set we held out at the beginning of our analysis pipeline. What performance do you get? Is that what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy, Monitor, and Maintain Your System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage you could think of wrapping the chosen model within a dedicated web service that your web application can access through a Web API. But this is the subject for another course!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of the demo for Week 2!!\n",
    "See you next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
