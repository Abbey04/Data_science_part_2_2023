{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Convolutional Neural Networks: an Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥ 3.8 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 8)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version', tf.__version__)\n",
    "print('Keras version', keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A quick overview on Convolutions\n",
    "\n",
    "To explain how convolutions works we will load a couple of sample images:\n",
    "* the image of a china temple\n",
    "* the image of a flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print out our images and see how they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(china)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create three 2D convolutional filters.\n",
    "\n",
    "Each filter will be $7 \\times 7$ in size and applied to all the channels in the image (our images are RGB, so we have three channels).\n",
    "\n",
    "The filters are:\n",
    "* a vertical bar filter\n",
    "* a horizontal bar filter\n",
    "* a filter consisting of a single positive central pixel surrounded by negative pixels. This, as we will see, behaves as an edge detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 7x7xn_channels filters, the first one as a vertical line \n",
    "# the second one as a horizontal one\n",
    "\n",
    "# first we create a create a 4D array \n",
    "# 7 X 7 X number of channels X number of filters \n",
    "filters = np.zeros(shape=(7, 7, channels, 3), dtype=np.float32)\n",
    "\n",
    "# our first filters is a vertical line \n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "\n",
    "# our second filter is a horizontal line \n",
    "filters[3, :, :, 1] = 1  # horizontal line\n",
    "\n",
    "# our third filter is -1 everywhere except in the centre\n",
    "# where it is = 48 so that the sum of all the pixels is zero.\n",
    "# This will behaves as an \"edge filter\"\n",
    "edge_filter = -1.0*np.ones((7, 7))\n",
    "edge_filter[3, 3] = 48\n",
    "edge_filter = np.repeat(edge_filter[:, :, np.newaxis], 3, axis=2)\n",
    "edge_filter = tf.constant(edge_filter, shape=(7, 7, channels), dtype=tf.float32)\n",
    "\n",
    "filters[:, :, :, 2] = edge_filter  # edge filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print out our three convolutional filters and see how they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(filters[:, :, 0, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(filters[:, :, 0, 1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(filters[:, :, 0, 2], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters[:, :, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform the 2D convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.nn.conv2d(\n",
    "    images,\n",
    "    filters,\n",
    "    strides=1,\n",
    "    padding=\"SAME\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple of functions to plot greyscale and colour images\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# crop images\n",
    "def crop(images):\n",
    "    return images[150:220, 130:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_index in (0, 1):\n",
    "    for feature_map_index in (0, 1, 2):\n",
    "        plt.subplot(2, 3, image_index * 3 + feature_map_index + 1)\n",
    "        plot_image(outputs[image_index, :, :, feature_map_index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter 1: Vertical line detection\n",
    "\n",
    "See this detail of the Chinese temple picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(crop(outputs[0, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter 2: Horizontal line detection\n",
    "\n",
    "See this detail of the Chinese temple picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(crop(outputs[0, :, :, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter 3:Edge detection\n",
    "\n",
    "This is very analogous to the behaviour of some cells in the Primary Visual Area of the Primates Brain (area V1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(outputs[0, :, :, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(outputs[1, :, :, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional and pooling layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Convolutional Layer\n",
    "\n",
    "In convolutional layers, the filters are not pre-defined/hard-coded but they are learned during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = keras.layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding=\"SAME\",\n",
    "    activation=\"relu\"\n",
    ")\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
    "max_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A Convolutional Network for the Fashion MNIST dataset (image classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data; create training, test and validation sets\n",
    "(\n",
    "    X_train_full, y_train_full\n",
    "), (\n",
    "    X_test, y_test\n",
    ") = keras.datasets.fashion_mnist.load_data()\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "# apply standard scaling to our data\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# add one inner dimension to the dataset so that it becomes 4-dimensional\n",
    "# 2D Convolutional networks take 4D tensors as inputs with shape (n_samples, width, height, n_channels)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 01:** Let us implement a simple CNN to tackle the Fashion MNIST dataset. We will implement a CNN with these requirements:\n",
    "  * a first 2-D Convolutional layer with 64 7x7 kernel filters, ReLU activation, and zero padding. Remember that the imput shape must be of the size of a single frame of the input image ($width \\times height \\times n_{channels}$) \n",
    "  * a max-pooling layer with pool size of 2\n",
    "  * a second 2-D Convolutional layer with 128 3x3 kernel, ReLU activation, and zero padding.\n",
    "  * a third 2-D Convolutional layer with 128 3x3 kernel, ReLU activation, and zero padding.\n",
    "  * a max-pooling layer with pool size of 2\n",
    "  * Now we repeat the same structure again: two more 2-D Convolutional layers with 256 3x3 kernel, ReLU activation, and zero padding, followed by one more MaxPooling layer with pool size of 2\n",
    "  * After that, stack a fully connected network, composed of two hidden dense layers and a dense output layer. you must flatten the inputs to the first dense layer, since a dense network expects a 1D array of features for each instance. Furthermore, add two dropout layers to the Dense hidden layers, with a dropout rate of 50% each, to reduce overfitting. The first dense layer will have 128 neurons, the second hidden layer will have 64.\n",
    "\n",
    "The number of kernel filters grows as we go up in the CNN towards the output layer: it is initially 64, then 128, then 256. It makes sense to have it increasing: the number of low-level visual feature is generally fairly low (e.g., edges, small circles...), but there are various ways to use them to generate higher-level features. Doubling the number of filters after each pooling layer is a common practice. A pooling layer divides each spatial dimension by a factor of 2, so we can afford to double the number of feature maps in the next layer without parameter explosion and increase of memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here\n",
    "n_net = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the summary of your model\n",
    "n_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 02:** Now let's compile and train the model. Use the correct loss function, the NADAM optimizer, and \"accuracy\" as a metric. Train the network ideally for at least 10 epochs.\n",
    "If the model takes too much to compile you can consider increasing the stride, or reducing the number of filters. This will likely affect your accuracy. On my 2019 MacBookPro each epoch takes about 6 minutes to complete, so expect a full training session of 10 epochs to last 1 hour or more. Once you have successfully trained your model you can save it for later usage/deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile your model here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model here and return the history object:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can evaluate the performance on the test set, if we are happy with the training result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell should work as it is if the previous ones have been coded correctly\n",
    "score = n_net.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:10] # pretend we have new images\n",
    "y_pred = n_net.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 03:** How can print out the predicted classes? Do it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa9cb710efc85fb5f4b4032a69aa40c8e866725b3bb56f2d012c19d5f6f7f57e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
